\section{Reversed Markov chain}

\begin{observation}
Reversed sequence of $ \mathbb{X} $ is also a Markov chain.
\begin{proof}
\begin{eqnarray*}
&   & P(Y(n) = i_{0} \mid Y(n - 1) = i_{1}, \cdots, Y(n - k) = i_{k}) \\
& = & P(X(m) = i_{0} \mid X(m + 1) = i_{1}, \cdots, X(m + k) = i_{k}) \\
& = & \frac{P(X(m) = i_{0}, X(m + 1) = i_{1}, \cdots, X(m + k) = i_{k})}{P(X(m + 1) = i_{1}, \cdots, X(m + k) = i_{k})} \\
& = & \frac{P(X(m) = i_{0}, X(m + 1) = i_{1}) \cdot P(X(m + 2) = i_{2}, \cdots, X(m + k) = i_{k} \mid X(m) = i_{0}, X(m + 1) = i_{1})}{P(X(m + 1) = i_{1}) \cdot P(X(m + 2) = i_{2}, \cdots, X(m + k) = i_{k} \mid X(m + 1) = i_{1})} \\
& = & P(X(m) = i_{0} \mid X(m + 1) = i_{1}) \\
& = & P(Y(n) = i_{0} \mid Y(n - 1) = i_{1}).
\end{eqnarray*}
\end{proof}
\end{observation}

\begin{observation} \label{obs:pi_i-q_ij-eq-pi_j-p_ji}
Let $ \mathbb{Y} $ be the reversed sequence of $ \mathbb{X} $, and let $ \boldsymbol\pi $ be a stationary distribution of $ \mathbb{X} $. Then, for all $ i, j \in \mathcal{S} $,
\[ \pi_{i} \cdot \mathbb{Q}[i, j] = \pi_{j} \cdot \mathbb{P}[j, i], \]
where $ \mathbb{Q} $ is the transition matrix of $ \mathbb{Y} $.
\begin{proof}
Suppose $ \mathbb{X} $ is in the steady-state distribution $ \boldsymbol\pi $, and hence $ \mathbb{Y} $ is also in the steady-state distribution $ \boldsymbol\pi $ for all time indices $ t_{i} $. Then,
\begin{eqnarray*}
\pi_{i} \cdot \mathbb{Q}[i, j]
  & = & P(Y(n - 1) = i) \cdot P(Y(n) = j \mid Y(n - 1) = i) \\
  & = & P(Y(n - 1) = i, Y(n) = j) \\
  & = & P(X(m + 1) = i, X(m) = j) \\
  & = & P(X(m) = j) \cdot P(X(m + 1) = i \mid X(m) = j) \\
  & = & \pi_{j} \cdot \mathbb{P}[j, i].
\end{eqnarray*}
\end{proof}
\end{observation}

\begin{observation}
$ \mathbb{X}: \mathbb{P}, \mathbb{Y}: \mathbb{Q} $.
If $ \boldsymbol\pi $ with $ \sum_{i \in \mathcal{S}} \pi_{i} = 1 $ satisfies $ \pi_{i} \cdot \mathbb{Q}[i, j] = \pi_{j} \cdot \mathbb{P}[j, i] $, then $ \mathbb{Y} $ is the reversed sequence of $ \mathbb{X} $.
\begin{proof}
For all $ i \in \mathcal{S} $,
\begin{eqnarray*}
\pi_{i}
  & = & \pi_{i} \underbrace{\sum_{j \in \mathcal{S}} \mathbb{Q}[i, j]}_{= 1} \\
  & = & \sum_{j \in \mathcal{S}} \pi_{i} \cdot \mathbb{Q}[i, j] \\
  & = & \sum_{j \in \mathcal{S}} \pi_{j} \cdot \mathbb{P}[j, i].
\end{eqnarray*}

That is, $ \boldsymbol\pi = \boldsymbol\pi \mathbb{P} $ and hence $ \boldsymbol\pi $ is a stationary distribution of $ \mathbb{X} $. By \autoref{obs:pi_i-q_ij-eq-pi_j-p_ji}, the transition matrix of the reversed sequence of $ \mathbb{X} $, $ \hat{\mathbb{Q}} $ satisfies
  \[ \hat{\mathbb{Q}}[i, j] = \frac{\pi_{j}}{\pi_i} \cdot \mathbb{P}[j, i] = \mathbb{Q}[i, j] \]
for all $ i, j \in \mathcal{S} $.
\end{proof}
\end{observation}

\begin{definition}
If there exists a stationary distribution $ \boldsymbol\pi $ of $ \mathbb{X} $ such that
\[ \pi_{i} \cdot \mathbb{Q}[i, j] = \pi_{j} \cdot \mathbb{P}[j, i] \]
for all $ i, j \in \mathcal{S} $,
then we say that $ \mathbb{Y} $ is a \defterm{reversed Markov chain} of $ \mathbb{X} $.
\end{definition}

\begin{definition}
$ \mathbb{X} $: $ \mathbb{P} $ is \defterm{time reversible} if there exists $ \boldsymbol\pi $ with $ \sum_{j \in \mathcal{S}} \pi_{j} = 1 $ such that
\[ \pi_{i} \cdot \mathbb{P}[i, j] = \pi_{j} \cdot \mathbb{P}[j, i] \]
for all $ i, j \in \mathcal{S} $.
\end{definition}

\begin{question}
\begin{figure}[htp]
\centering
\begin{tikzpicture}
  \node[state] (Xi-1)                             {$ i - 1 $};
  \node[state] (Xi)   [right of=Xi-1]             {$ i $};
  \node[state] (Xi+1) [right of=Xi]               {$ i + 1 $};
  \node[state] (X0)   [left  of=Xi-1,xshift=-2em] {$ 0 $};
  \node[state] (Xm)   [right of=Xi+1,xshift=2em]  {$ m $};

  \path (X0) edge [loop left]     node [swap] {$ 1 - p_{0} $} (X0)
        (Xi) edge [bend right=40] node [swap] {$ 1 - p_{i} $} (Xi-1)
        (Xi) edge [bend left=40]  node        {$ p_{i} $} (Xi+1)
        (Xm) edge [loop right]    node        {$ p_{m} $} (Xm);
  \path[-] (X0) edge [loosely dotted,line width=.15em] (Xi-1)
           (Xm) edge [loosely dotted,line width=.15em] (Xi+1);
\end{tikzpicture}
\[ 0 < p_{0} \le 1, ~ 0 \le p_{m} < 1, ~ 0 < p_{i} < 1 ~ \forall i = 2, \cdots, m - 1 \]
\[ p_{i} = \frac{m - i}{m} \]
\end{figure}

Is the Markov chain above time-reversible?
\begin{eqnarray*}
\pi_{i}
  & = & \prod_{j = 1}^{i} \frac{p_{j - 1}}{1 - p_{j}} \cdot \pi_{0} \\
  & = & \frac{g_{i}}{1 + \sum_{i = 1}^{m} g_{i}} \\
  & = & g_{i} \cdot \pi_{0} \\
  & = & \frac{1}{2^{m}} \begin{pmatrix} m \\ i \end{pmatrix},
\end{eqnarray*}
where $ g_{i} = \begin{pmatrix} m \\ i \end{pmatrix} $.
\end{question}

\begin{question}
\begin{figure}[htp]
\centering
\begin{tikzpicture}[node distance=6em]
  \node[state] (X2)               {$ 2 $};
  \node[state] (X1) [right of=X2] {$ 1 $};
  \node[state] (X4) [right of=X1] {$ 4 $};
  \node[state] (X5) [right of=X4] {$ 5 $};
  \node[state] (X3) [right of=X5] {$ 3 $};

  \path[-] (X2) edge [bend right=0] node {$ 3 $} (X1)
           (X1) edge [bend right=0] node {$ 1 $} (X4)
           (X4) edge [bend right=0] node {$ 4 $} (X5)
           (X5) edge [bend right=0] node {$ 6 $} (X3)
           (X1) edge [bend left=50] node {$ 2 $} (X5);
\end{tikzpicture}
\[ \mathbb{P}[i, j] = \frac{w_{ij}}{\sum_{k = 1}^{n} w_{ik}} \]
\end{figure}

What is the stationary distribution of $ \mathbb{X} $? Is $ \mathbb{X} $ time-reversible?
\[ \pi_{i} = \frac{\sum_{k = 1}^{n} w_{ik}}{\sum_{l = 1}^{n} \sum_{k = 1}^{n} w_{lk}}. \]
\begin{eqnarray*}
\pi_{i} \mathbb{P}[i, j]
  & = & \frac{w_{ij}}{\sum_{l = 1}^{n} \sum_{k = 1}^{n} w_{lk}} \\
  & = & \frac{\sum_{k = 1}^{n} w_{jk}}{\sum_{l = 1}^{n} \sum_{k = 1}^{n} w_{lk}} \cdot \frac{w_{ji}}{\sum_{k = 1}^{n} w_{jk}} \\
  & = & \pi_{j} \mathbb{P}[j, i].
\end{eqnarray*}

Thus, $ \mathbb{X} $ is time-reversible and $ \pi = (\frac{6}{32}, \frac{3}{32}, \frac{6}{32}, \frac{5}{32}, \frac{12}{32}) $.
\end{question}

\subsection{Hastings-Metropolis Sampling Algorithm}
Given a random variable $ X $ with probability mass function $ P(\cdot) $ compute an irreducible (finite) Markov chain $ \mathbb{X} $ such that the unique long-run proportion vector equals the probability distribution of $ X $. That is,
\[ \lim_{n \to \infty} \frac{1}{n} \sum_{i = 1}^{n} X(i) = \sum_{i \in \mathcal{S}} i \underbrace{\cdot P(X = i)}_{= r_{i}} = E[X]. \]

We only need to know $ b_{i} $ for all $ i \in \mathcal{S} $ such that $ P(X = i) = b_{i} / C $ where $ C $ is a unknown (or intractible) constant.

\subsubsection{Mathematical definition of $ \mathbb{X} : \mathbb{P} $}
\begin{enumerate}
  \item Let $ \mathbb{Q} $ be the transition matrix of an arbitrary irreducible Markov chain over the same space of $ \mathbb{X} $ (This is what we have to ``design'').
  \item Probability $ q(i, j) $ for any two states $ i, j \in \mathcal{S} $ is to be determined later (This depends on $ \mathbb{Q} $ and $ \boldsymbol\pi $).
\end{enumerate}
\begin{eqnarray*}
\mathbb{P}[i, j] & = &
  \begin{cases}
    \mathbb{Q}[i, j] \cdot q(i, j)                        & i \neq j, \\
    \mathbb{Q}[i, j] \cdot (1 - \sum_{i \neq k} q(i, k))  & i = j,
  \end{cases} \\
q(i, j) & = & \min \left\{ \frac{b_{j} \cdot \mathbb{Q}[j, i]}{b_{i} \cdot \mathbb{Q}[i, j]}, 1 \right\}.
\end{eqnarray*}

\subsubsection{Algorithmic definition of $ \mathbb{X} : \mathbb{P} $}
\begin{itemize}
  \item \textbf{Repeat for} $ n = 0, 1, \cdots $
    \begin{description}
      \item[Step 1.] Let $ i = X(n) $. \\
        Set $ Y(n) $ according to the probability specified by the $ i $-th row of $ \mathbb{Q} $. \\
        That is, $ P(Y(n) = j) = \mathbb{Q}[i, j] $ for all $ j \in \mathcal{S} $.
      \item[Step 2.] Let $ j = Y(n) $. \\
        Set $ X(n + 1) $ to be $ j $ with probability $ q(i, j) $, and to be $ i $ with probability $ 1 - q(i, j) $.
    \end{description}
\end{itemize}

\begin{example}
Let $ \mathcal{S} $ be the state space consisting of the permutations $ \mathbf{x} = (x_{1}, \cdots, x_{m}) $ over $ \{ 1, \cdots, n \} $ with $ \sum_{i \in \mathcal{S}} i \cdot x_{i} \ge n^{3} / 4 $. The objective is to generate uniform random samples over $ \mathcal{S} $. That is, for each $ \mathbf{x} \in \mathcal{S} $ and each $ t $, we except $ P(X(t) = \mathbf{x}) = 1 / |\mathcal{S}| $ ($ \forall \mathbf{x} \in \mathcal{S}. b_{\mathbf{x}} = 1 $, $ C = |\mathcal{S}| $).

To make $ \mathbb{X} $ time-reversible with $ \boldsymbol\pi $ being its unique stationary distribution, it suffices to design (from $ \mathbb{Q} $) the matrix $ q $ such that
\[ \pi_{i} \cdot \mathbb{P}[i, j] = \pi_{j} \cdot \mathbb{P}[j, i] \]
holds for all $ i \neq j $. That is,
\[ \frac{b_{i}}{C} \cdot \mathbb{Q}[i, j] \cdot q(i, j) = \frac{b_{j}}{C} \cdot \mathbb{Q}[j, i] \cdot q(j, i). \]

\begin{description}
  \item[Case 1.]
    \[ \frac{b_{j} \cdot \mathbb{Q}[j, i]}{b_{i} \cdot \mathbb{Q}[i, j]} \ge 1
      \Rightarrow q(i, j) = 1, q(j, i) = \frac{b_{i} \cdot \mathbb{Q}[i, j]}{b_{j} \cdot \mathbb{Q}[j, i]}. \]
  \item[Case 2.]
    \[ \frac{b_{j} \cdot \mathbb{Q}[j, i]}{b_{i} \cdot \mathbb{Q}[i, j]} < 1,
      \Rightarrow q(i, j) = \frac{b_{j} \cdot \mathbb{Q}[j, i]}{b_{i} \cdot \mathbb{Q}[i, j]}, q(j, i) = 1. \]
\end{description}

For any two states $ \mathbf{x} $ and $ \mathbf{y} $ in $ \mathcal{S} $, that can be obtained from each other via one swap of adjacent elements, let
\[ \mathbb{Q}[\mathbf{x}, \mathbf{y}] = \frac{1}{N(\mathbf{x})}, \]
where $ N(\mathbf{x}) $ is the number of permutations in $ \mathcal{S} $ that can be obtained from $ \mathbf{x} $ via one swap of adjacent elements.
\end{example}
